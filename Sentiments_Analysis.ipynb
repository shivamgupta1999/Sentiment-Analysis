{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiments Analysis - Twitter Dataset\n",
    "\n",
    "### Author: [Shivam Gupta](https://www.linkedin.com/in/shivamgupta1999/)\n",
    "\n",
    "* Classifying whether a tweet has a **positive** sentiment or a **negative** sentiment.\n",
    "* Preprocessing of dataset includes:\n",
    "  * Removing of punctuations and stopwords (like- and, the ,is...)\n",
    "  * Removing hyperlinks and hashtags\n",
    "  * Tokenizing the tweet\n",
    "  * stemming the tokens\n",
    "* Logistic Regression model is used and implemented from scratch without using any libraries.\n",
    "* Gradient Descent algorithm is used to minimize the loss and this is also implemented in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Downloading the dataset\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y.shape = (8000, 1)\n",
      "test_y.shape = (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "positive_tweets=twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets=twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "#There are 5000 tweets each of positive and negative sentiments\n",
    "#Splitting dataset into train and test in ratio 4:1\n",
    "train_pos = positive_tweets[:4000]\n",
    "test_pos = positive_tweets[4000:]\n",
    "train_neg = negative_tweets[:4000]\n",
    "test_neg = negative_tweets[4000:]\n",
    "\n",
    "train_x = train_pos + train_neg \n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "#Preparing labels\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)\n",
    "\n",
    "print(\"train_y.shape = \" + str(train_y.shape))\n",
    "print(\"test_y.shape = \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we'll define a function doing all the preprocessing task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \n",
    "    #remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    #only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    #tokenizing the tweet\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    #Stemming the word and removing stopwords and punctuations\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    \n",
    "    tweets_clean = [stemmer.stem(word) for word in tweet_tokens if word not in stopwords_english and word not in string.punctuation]\n",
    "    \n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function will return a dictionary mapping each word-sentiment pair to it's frequency in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_dict(tweets, sentiments):\n",
    "    \n",
    "    labels = np.squeeze(sentiments).tolist()\n",
    "\n",
    "    frequencies = {}\n",
    "    for label, tweet in zip(labels, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, label)\n",
    "            if pair in frequencies:\n",
    "                frequencies[pair] += 1\n",
    "            else:\n",
    "                frequencies[pair] = 1\n",
    "\n",
    "    return frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function which provides the feature vector having frequencies of positive and negative tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_vector(tweet, frequencies):\n",
    "    \n",
    "    # cleaning the tweet by removing puntuations and stopwords, tokenizing and stemming\n",
    "    clean_tweet = process_tweet(tweet)\n",
    "    \n",
    "    # feature vector containing a bias term and positive and negative tokes's frequencies\n",
    "    features = np.zeros((1, 3)) \n",
    "    \n",
    "    #bias term is set to 1\n",
    "    features[0,0] = 1 \n",
    "    \n",
    "    for word in clean_tweet:\n",
    "        # increment the word count for the positive token\n",
    "        features[0,1] += frequencies.get((word,1),0)\n",
    "        \n",
    "        # increment the word count for the negative token\n",
    "        features[0,2] += frequencies.get((word,0),0)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying preprocessing functions on actual dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(frequencies) = 11345\n",
      "This is an example of a positive tweet: \n",
      " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "This is an example of the processed version of the tweet: \n",
      " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "frequencies = frequency_dict(train_x, train_y)\n",
    "\n",
    "print(\"len(frequencies) = \" + str(len(frequencies.keys())))\n",
    "\n",
    "print('This is an example of a positive tweet: \\n', train_x[0])\n",
    "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting each tweet to corresponding feature vector\n",
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :]= feature_vector(train_x[i], frequencies)\n",
    "\n",
    "# training labels corresponding to X\n",
    "Y = train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z): \n",
    "    \n",
    "    sig = 1/(1+np.exp(-z))\n",
    "    \n",
    "    return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Gradient Descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, theta, LR, iterations):\n",
    "    \n",
    "    m = x.shape[0]\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        z = np.dot(x,theta)\n",
    "        sig = sigmoid(z)\n",
    "        cost = (-1./m)*(np.dot(y.transpose(),np.log(sig))+np.dot((np.ones(y.shape)-y).transpose(),np.log(np.ones(y.shape)-sig)))\n",
    "        theta -= (LR/m)*(np.dot(x.transpose(),sig-y))\n",
    "        \n",
    "    cost = float(cost)\n",
    "    return cost, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after training is 0.24217.\n",
      "The resulting vector of weights is [0.0, 0.00052, -0.00056]\n"
     ]
    }
   ],
   "source": [
    "# Apply gradient descent\n",
    "cost, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500) #Learning rate is set to 1e-9 and 1500 iterations\n",
    "print(f\"The cost after training is {cost:.5f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 5) for t in np.squeeze(theta)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, frequencies, theta):\n",
    "    \n",
    "    # extracting features of the tweet\n",
    "    x = feature_vector(tweet, frequencies)\n",
    "    \n",
    "    # Prediction using x and theta\n",
    "    y_pred = sigmoid(np.dot(x,theta))\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "def logistic_regression(test_x, test_y, frequencies, theta):\n",
    "\n",
    "    m=test_y.shape[0]\n",
    "    \n",
    "    # Predictions\n",
    "    y_hat = []\n",
    "    \n",
    "    for tweet in test_x:\n",
    "        # get the label prediction for the tweet\n",
    "        y_pred = predict_tweet(tweet, frequencies, theta)\n",
    "        \n",
    "        if y_pred > 0.5:\n",
    "            y_hat.append(1.0)  # Positive sentiment\n",
    "        else:\n",
    "            y_hat.append(0)    # Negative sentiment\n",
    "\n",
    "    # Calculating accuracy\n",
    "    y_hat = np.asarray(y_hat)\n",
    "    test_y = np.squeeze(test_y)\n",
    "    acc = y_hat==test_y\n",
    "    accuracy = np.sum(acc)/m\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting sentiments of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model's accuracy = 0.9950\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = logistic_regression(test_x, test_y, frequencies, theta)\n",
    "print(f\"Logistic regression model's accuracy = {test_accuracy:.4f}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our model is predicting sentiments with 99.5% Accuracy!!! Isn't that great? :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can also test this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive sentiment\n"
     ]
    }
   ],
   "source": [
    "# Replace my_tweet with any sentence you like and check the prediction\n",
    "\n",
    "my_tweet = \"Christian bale's performance was brilliant in Dark knight!\"\n",
    "\n",
    "y_hat = predict_tweet(my_tweet, frequencies, theta)\n",
    "if y_hat > 0.5:\n",
    "    print('Positive sentiment')\n",
    "else: \n",
    "    print('Negative sentiment')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
